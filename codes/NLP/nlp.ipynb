{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./jupyter_env/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: pandas in ./jupyter_env/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in ./jupyter_env/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: nltk in ./jupyter_env/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: vaderSentiment in ./jupyter_env/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: transformers in ./jupyter_env/lib/python3.8/site-packages (4.41.2)\n",
      "Requirement already satisfied: gensim in ./jupyter_env/lib/python3.8/site-packages (4.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: xxhash in ./jupyter_env/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: packaging in ./jupyter_env/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: multiprocess in ./jupyter_env/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./jupyter_env/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: aiohttp in ./jupyter_env/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: filelock in ./jupyter_env/lib/python3.8/site-packages (from datasets) (3.15.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in ./jupyter_env/lib/python3.8/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./jupyter_env/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./jupyter_env/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./jupyter_env/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./jupyter_env/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./jupyter_env/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./jupyter_env/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./jupyter_env/lib/python3.8/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: click in ./jupyter_env/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./jupyter_env/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./jupyter_env/lib/python3.8/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./jupyter_env/lib/python3.8/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jupyter_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./jupyter_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupyter_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupyter_env/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./jupyter_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./jupyter_env/lib/python3.8/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./jupyter_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./jupyter_env/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./jupyter_env/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in ./jupyter_env/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./jupyter_env/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./jupyter_env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: wrapt in ./jupyter_env/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets pandas scikit-learn nltk vaderSentiment transformers gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rating': 5.0, 'title': 'Pretty locket', 'text': 'I think this locket is really pretty. The inside back is a solid silver depression and the front is a dome that is not solid (knotted). You could use it to store a small photo, lock of hair, etc but I use it when I need to carry medication with me. Closes securely. High quality & very pretty.', 'images': [], 'asin': 'B00LOPVX74', 'parent_asin': 'B00LOPVX74', 'user_id': 'AGBFYI2DDIKXC5Y4FARTYDTQBMFQ', 'timestamp': 1578528394489, 'helpful_vote': 3, 'verified_purchase': True}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Amazon_Fashion\", trust_remote_code=True)\n",
    "print(dataset[\"full\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  rating\n",
      "0  i think this locket is really pretty the insid...     5.0\n",
      "1                                              great     5.0\n",
      "2  one of the stones fell out within the first 2 ...     2.0\n",
      "3  crappy socks money wasted bought to wear with ...     1.0\n",
      "4  i love these glasses  they fit perfectly over ...     5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Convert to Pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(dataset['full'])\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['text', 'rating'], inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=['text', 'user_id'], inplace=True)\n",
    "\n",
    "# Preprocess text (lowercasing, removing punctuation)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(df[['cleaned_text', 'rating']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/dgxuser55/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  sentiment  \\\n",
      "0  i think this locket is really pretty the insid...     0.8532   \n",
      "1                                              great     0.6249   \n",
      "2  one of the stones fell out within the first 2 ...     0.0000   \n",
      "3  crappy socks money wasted bought to wear with ...    -0.6908   \n",
      "4  i love these glasses  they fit perfectly over ...     0.9516   \n",
      "\n",
      "  sentiment_category  \n",
      "0           positive  \n",
      "1           positive  \n",
      "2            neutral  \n",
      "3           negative  \n",
      "4           positive  \n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment\n",
    "df['sentiment'] = df['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Categorize sentiment\n",
    "def categorize_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)\n",
    "\n",
    "print(df[['cleaned_text', 'sentiment', 'sentiment_category']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your local CSV file\n",
    "csv_file_path = r\"fake reviews dataset.csv\"\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "fake_review_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(fake_review_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the dataset for BERT\n",
    "fake_review_df['text'] = fake_review_df['text_'].apply(preprocess_text)\n",
    "X = fake_review_df['text']\n",
    "y = fake_review_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.head(), y_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/raid/home/dgxuser55/jupyter_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training set: 5000\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 157it [24:23,  9.32s/it]                                          \n",
      "Validation: 32it [01:20,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9580000042915344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, InputExample, InputFeatures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to preprocess text (define this as needed)\n",
    "def preprocess_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "csv_file_path = r\"fake reviews dataset.csv\"\n",
    "fake_review_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Remove rows with NaN values\n",
    "fake_review_df.dropna(subset=['text_', 'label'], inplace=True)\n",
    "\n",
    "# Preprocess the text data\n",
    "fake_review_df['text'] = fake_review_df['text_'].apply(preprocess_text)\n",
    "\n",
    "# Prepare the dataset for BERT\n",
    "X = fake_review_df['text']\n",
    "y = fake_review_df['label']\n",
    "\n",
    "# Split the data into training and testing sets (using a smaller subset for quick testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Use a smaller subset for faster training\n",
    "X_train = X_train[:5000]\n",
    "y_train = y_train[:5000]\n",
    "X_test = X_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "# Convert labels from CG and OR to 0 and 1\n",
    "label_map = {'CG': 0, 'OR': 1}\n",
    "y_train = y_train.map(label_map)\n",
    "y_test = y_test.map(label_map)\n",
    "\n",
    "# Verify no NaN values are present\n",
    "assert y_train.isna().sum() == 0, \"NaN values found in y_train\"\n",
    "assert y_test.isna().sum() == 0, \"NaN values found in y_test\"\n",
    "\n",
    "train_data = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "test_data = pd.DataFrame({'text': X_test, 'label': y_test})\n",
    "\n",
    "# Function to convert data to InputExamples\n",
    "def convert_data_to_examples(train, test):\n",
    "    train_input_examples = train.apply(lambda x: InputExample(guid=None, \n",
    "                                                              text_a=x['text'], \n",
    "                                                              text_b=None, \n",
    "                                                              label=x['label']), axis=1)\n",
    "\n",
    "    validation_input_examples = test.apply(lambda x: InputExample(guid=None, \n",
    "                                                                  text_a=x['text'], \n",
    "                                                                  text_b=None, \n",
    "                                                                  label=x['label']), axis=1)\n",
    "\n",
    "    return train_input_examples, validation_input_examples\n",
    "\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(train_data, test_data)\n",
    "\n",
    "# Function to convert examples to TensorFlow dataset\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] \n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids, token_type_ids = input_dict[\"input_ids\"], input_dict[\"token_type_ids\"]\n",
    "        attention_mask = input_dict[\"attention_mask\"]\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label)\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\n",
    "            \"input_ids\": tf.int32,\n",
    "            \"attention_mask\": tf.int32,\n",
    "            \"token_type_ids\": tf.int32,\n",
    "        },\n",
    "        tf.int64),\n",
    "        ({\n",
    "            \"input_ids\": tf.TensorShape([None]),\n",
    "            \"attention_mask\": tf.TensorShape([None]),\n",
    "            \"token_type_ids\": tf.TensorShape([None]),\n",
    "        },\n",
    "        tf.TensorShape([])),\n",
    "    )\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Convert InputExamples to TensorFlow Datasets\n",
    "train_dataset = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "train_dataset = train_dataset.shuffle(100).batch(32)  # Removed .repeat(2)\n",
    "\n",
    "validation_dataset = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "validation_dataset = validation_dataset.batch(32)\n",
    "\n",
    "# Compile the model using the legacy Adam optimizer\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=3e-5, epsilon=1e-08, decay=0.01, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Calculate the length of the training set\n",
    "len_X_train = len(X_train)\n",
    "print(f\"Length of the training set: {len_X_train}\")\n",
    "\n",
    "# Calculate the total number of batches per epoch and remaining iterations\n",
    "average_time_per_iteration = 6.44  # seconds\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate total batches per epoch\n",
    "total_batches = len_X_train // batch_size\n",
    "\n",
    "# Use a custom training loop with tqdm for progress bar\n",
    "epochs = 1  # Reduce the number of epochs for quicker training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # Train\n",
    "    for step, (batch_inputs, batch_labels) in tqdm(enumerate(train_dataset), desc=\"Training\", total=total_batches):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(batch_inputs, training=True)[0]\n",
    "            loss_value = loss(batch_labels, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # Validation\n",
    "    for batch_inputs, batch_labels in tqdm(validation_dataset, desc=\"Validation\"):\n",
    "        logits = model(batch_inputs, training=False)[0]\n",
    "        loss_value = loss(batch_labels, logits)\n",
    "        metric.update_state(batch_labels, logits)\n",
    "\n",
    "    print(f\"Validation Accuracy: {metric.result().numpy()}\")\n",
    "    metric.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 4), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 2), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Preprocess text for LDA\n",
    "df['tokenized_text'] = df['cleaned_text'].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(df['tokenized_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokenized_text']]\n",
    "\n",
    "print(corpus[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.067*\"they\" + 0.049*\"them\" + 0.046*\"are\" + 0.045*\"these\" + 0.031*\"and\" + 0.018*\"my\" + 0.018*\"the\" + 0.015*\"to\" + 0.015*\"for\" + 0.014*\"were\"\n",
      "Topic: 1 \n",
      "Words: 0.056*\"my\" + 0.055*\"it\" + 0.052*\"for\" + 0.035*\"and\" + 0.033*\"was\" + 0.024*\"this\" + 0.023*\"as\" + 0.019*\"loved\" + 0.014*\"great\" + 0.014*\"she\"\n",
      "Topic: 2 \n",
      "Words: 0.064*\"the\" + 0.035*\"it\" + 0.029*\"and\" + 0.026*\"but\" + 0.024*\"is\" + 0.021*\"was\" + 0.020*\"not\" + 0.020*\"to\" + 0.017*\"size\" + 0.015*\"like\"\n",
      "Topic: 3 \n",
      "Words: 0.070*\"and\" + 0.046*\"love\" + 0.041*\"very\" + 0.038*\"it\" + 0.037*\"great\" + 0.033*\"the\" + 0.027*\"is\" + 0.025*\"good\" + 0.024*\"this\" + 0.023*\"quality\"\n",
      "Topic: 4 \n",
      "Words: 0.071*\"the\" + 0.032*\"to\" + 0.031*\"it\" + 0.025*\"and\" + 0.020*\"of\" + 0.016*\"is\" + 0.014*\"this\" + 0.013*\"in\" + 0.013*\"on\" + 0.012*\"that\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Set parameters\n",
    "num_topics = 5\n",
    "passes = 1\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus, \n",
    "                     num_topics=num_topics, \n",
    "                     id2word=dictionary, \n",
    "                     passes=passes)\n",
    "\n",
    "# Print topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sentiment_results \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_category\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fake Review Detection\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m fake_review_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      6\u001b[0m fake_review_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(validation_dataset)\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Topic Modeling Results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis Results\n",
    "sentiment_results = df[['cleaned_text', 'sentiment_category']]\n",
    "\n",
    "# Fake Review Detection\n",
    "fake_review_results = test_data[['text']]\n",
    "fake_review_results['predicted_label'] = model.predict(validation_dataset).logits.argmax(axis=1)\n",
    "\n",
    "# Topic Modeling Results\n",
    "topic_results = [lda_model[dictionary.doc2bow(text)] for text in df['tokenized_text']]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\\n\", sentiment_results.head())\n",
    "print(\"Fake Review Detection Results:\\n\", fake_review_results.head())\n",
    "print(\"Topic Modeling Results:\\n\", topic_results[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save models\n",
    "joblib.dump(sia, 'vader_sentiment_analyzer.pkl')\n",
    "joblib.dump(model, 'bert_fake_review_detector.pkl')\n",
    "lda_model.save('lda_topic_model.model')\n",
    "\n",
    "# Load models\n",
    "sia = joblib.load('vader_sentiment_analyzer.pkl')\n",
    "model = joblib.load('bert_fake_review_detector.pkl')\n",
    "lda_model = LdaModel.load('lda_topic_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VADER sentiment analyzer\n",
    "sia = joblib.load('vader_sentiment_analyzer.pkl')\n",
    "\n",
    "# Analyze sentiment for your dataset\n",
    "df['sentiment'] = df['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)\n",
    "\n",
    "# Function to categorize sentiment\n",
    "def categorize_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "print(df[['cleaned_text', 'sentiment', 'sentiment_category']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Function to preprocess and tokenize the text for BERT\n",
    "def preprocess_for_bert(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "# Prepare the dataset for prediction\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "\n",
    "for text in df['cleaned_text']:\n",
    "    encoded_dict = preprocess_for_bert(text)\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = tf.concat(input_ids, axis=0)\n",
    "attention_masks = tf.concat(attention_masks, axis=0)\n",
    "token_type_ids = tf.concat(token_type_ids, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained LDA model and dictionary\n",
    "lda_model = LdaModel.load('lda_topic_model.model')\n",
    "dictionary = joblib.load('lda_dictionary.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "df['tokenized_text'] = df['cleaned_text'].apply(gensim.utils.simple_preprocess)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokenized_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer topics for each document\n",
    "df['topics'] = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "\n",
    "print(df[['cleaned_text', 'topics']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display combined results\n",
    "results = df[['cleaned_text', 'sentiment_category', 'fake_review_pred', 'topics']]\n",
    "print(results.head())\n",
    "\n",
    "# Save the results to a CSV file for further analysis\n",
    "results.to_csv('nlp_analysis_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Prepare data (assuming df contains 'text' and 'label' columns)\n",
    "train_texts = df['cleaned_text'].tolist()\n",
    "train_labels = df['label'].tolist()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
